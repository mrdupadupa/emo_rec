# emo_rec
fun project to make a demo for the Long-night-of Science 
in the development stage

**The night sky is simulated using WebGL, while a Python script with DeepFace handles face detection via webcam. The AI-generated music adapts in real time to changes in people's emotions, syncing with the simulation.**

## versions

- **1.0** version without AI music

- **2.0** version with AI music (generated by Magneta from google) and improved performance (multiple faces support added). More faces create more sounds. Red frame added as a signal when emotion is captioning.


## how to run:

### simulation part

`cd space-3d/src`

`npm start`

### face detection part: open a new terminal 


please create a venv e.g

`python -m venv venv/env`  
`source venv/env/bin/activate`

`pip install -r requirements`

**you are ready to run the inference part**

go to the root directory of the repo

run one of the the scripts:

- `python emo_rec+sound.py`  - 1.0 version without AI music

- `python inference.py`  - 2.0 version with AI music and improved performance (multiple faces support)

**You should see:**

python script will find the simulation ip that running on the backgroud. You should see an image from a webcam and separately webpage with a simulation, when there is a face detected you should see change in the simulation and music

## to-do:

- change to normal API instead of seleniun
- work with performance on small machines
